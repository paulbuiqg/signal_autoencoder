{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's create test data: a batch of three 8-channel signals of different length."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "n_channel = 8\n",
    "\n",
    "x1 = torch.rand((1000, n_channel))  # Length: 1000\n",
    "x2 = torch.rand((2000, n_channel))  # Length: 2000\n",
    "x3 = torch.rand((3000, n_channel))  # Length: 3000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then gather the signals, pad them to the batch max length, store the original signal lengths, create a mask to recover later which elements are padded."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input tensor size: torch.Size([3, 3000, 8])\n"
     ]
    }
   ],
   "source": [
    "from dataloading import collate_fn\n",
    "\n",
    "x_in, lengths, mask = collate_fn([(x1, 1000), (x2, 2000), (x3, 3000)])\n",
    "print('Input tensor size:', x_in.size())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define the autoencoder model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from modeling import SignalAutoencoder\n",
    "\n",
    "n_conv_channel_1 = 64\n",
    "n_conv_channel_2 = 128\n",
    "n_conv_channel_3 = 256\n",
    "lstm_hidden_size = 512\n",
    "n_lstm_layer = 2\n",
    "\n",
    "autoenc = SignalAutoencoder(n_channel, n_conv_channel_1,\n",
    "                            n_conv_channel_2, n_conv_channel_3,\n",
    "                            lstm_hidden_size, n_lstm_layer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output tensor size: torch.Size([3, 3000, 8])\n"
     ]
    }
   ],
   "source": [
    "x_out = autoenc(x_in)\n",
    "\n",
    "print('Output tensor size:', x_out.size())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compute the L1 distance between the original input and the reconstructed output. Earlier variables `lengths` and `mask` are used here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "L1 distance: 3.994481325149536\n"
     ]
    }
   ],
   "source": [
    "from modeling import sequence_l1\n",
    "\n",
    "err = sequence_l1(x_in, x_out, lengths, mask)\n",
    "\n",
    "print('L1 distance:', err.item())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This distance can be used as a loss for training the autoencoder."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, get the codes (compressed representations) corresponding to the batch input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Code size: torch.Size([3, 1024])\n"
     ]
    }
   ],
   "source": [
    "code = autoenc.encode(x_in)\n",
    "print('Code size:', code.size())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The latent space dimension, i.e. the code size, is `lstm_hidden_size * n_lstm_layer`, here `512 * 2 = 1024`."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
