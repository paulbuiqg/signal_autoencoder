{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import packages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "import torch\n",
    "\n",
    "module_path = os.path.abspath(os.path.join('..'))\n",
    "if module_path not in sys.path:\n",
    "    sys.path.append(module_path)\n",
    "\n",
    "from signal_autoencoder.dataloading import collate_fn\n",
    "from signal_autoencoder.modeling import (ConvAutoencoder,\n",
    "                                         ConvRecAutoencoder,\n",
    "                                         count_parameters,\n",
    "                                         sequence_l1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's create test data: a batch of three 8-channel signals of different length."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_channel = 8\n",
    "\n",
    "x1 = torch.rand((1000, n_channel))  # Length: 1000\n",
    "x2 = torch.rand((2000, n_channel))  # Length: 2000\n",
    "x3 = torch.rand((2999, n_channel))  # Length: 2999"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then gather the signals, pad them to the batch max length, store the original signal lengths, create a mask to recover later which elements are padded."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input tensor size: torch.Size([3, 2999, 8])\n"
     ]
    }
   ],
   "source": [
    "x_in, lengths, mask = collate_fn([(x1, 1000), (x2, 2000), (x3, 3000)])\n",
    "print('Input tensor size:', x_in.size())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define the convolutive autoencoder model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The model has 9362160 parameters\n"
     ]
    }
   ],
   "source": [
    "n_conv_channel_1 = 128\n",
    "n_conv_channel_2 = 256\n",
    "n_conv_channel_3 = 512\n",
    "n_conv_channel_4 = 1024\n",
    "\n",
    "autoenc = ConvAutoencoder(n_channel, n_conv_channel_1, n_conv_channel_2,\n",
    "                          n_conv_channel_3, n_conv_channel_4)\n",
    "\n",
    "print(f'The model has {count_parameters(autoenc)} parameters')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define the convolutive recurrent autoencoder model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The model has 8584432 parameters\n"
     ]
    }
   ],
   "source": [
    "n_conv_channel_1 = 64\n",
    "n_conv_channel_2 = 128\n",
    "n_conv_channel_3 = 256\n",
    "lstm_hidden_size = 512\n",
    "n_lstm_layer = 2\n",
    "\n",
    "autoenc = ConvRecAutoencoder(n_channel, n_conv_channel_1,\n",
    "                             n_conv_channel_2, n_conv_channel_3,\n",
    "                             lstm_hidden_size, n_lstm_layer)\n",
    "\n",
    "print(f'The model has {count_parameters(autoenc)} parameters')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output tensor size: torch.Size([3, 2992, 8])\n"
     ]
    }
   ],
   "source": [
    "x_out = autoenc(x_in)\n",
    "\n",
    "print('Output tensor size:', x_out.size())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compute the L1 distance between the original input and the reconstructed output. Earlier variables `lengths` and `mask` are used here, to discard padded elements from the loss computation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "L1 distance: 5.819143772125244\n"
     ]
    }
   ],
   "source": [
    "err = sequence_l1(x_in, x_out, lengths, mask)\n",
    "\n",
    "print('L1 distance:', err.item())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This distance can be used as a loss for training the autoencoder."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, get the codes (compressed representations) corresponding to the batch input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Code size: torch.Size([3, 1024])\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    _, (code, _) = autoenc.encoder(x_in)\n",
    "code = code.permute((1, 0, 2))  # Batch dimension first\n",
    "code = code.flatten(start_dim=1)\n",
    "print('Code size:', code.size())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the convolutional recurrent autoencoder, the latent space dimension, i.e. the code size, is `lstm_hidden_size * n_lstm_layer`, here `512 * 2 = 1024`."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "vscode": {
   "interpreter": {
    "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
